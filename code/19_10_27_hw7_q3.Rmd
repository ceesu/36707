---
title: "HW7 q3"
date: "10/7/2019"
output: pdf_document
---


```{r setup, include=FALSE}
# libraries
library(tidyverse)
library(lattice)
library(dagitty)
library(reshape2)
library(grid)
library(gridExtra)
library(np)
library(stargazer)
library(knitr)
suppressWarnings(suppressMessages(library(stargazer))) 
knitr::opts_chunk$set(warning=FALSE,width.cutoff=60, echo = TRUE)
opts_knit$set(eval.after = 'fig.cap')

#   This question uses the following dataset on airline flight delays: http://rosmarus.refsmmat.com/datasets/datasets/flight-delays/
#   Download DFW_ORD_2016_12.csv, the subset of the data that only contains Dallas/Fort Worth
# and Chicago O’Hare. Also, install and load the np package for nonparametric r=egression;
# the example file I used in class, kernel-smoothing-density.Rmd, is posted on Canvas under
# Files→Activities.

# data
data <- read.csv("../data/DFW_ORD_2016_12.csv",
                     sep = ",", 
                     header = T)

#  Also, make a “day of week” variable by taking the remainder of dividing “day of month” by 7
data$DAY_OF_WEEK <- data$DAY_OF_MONTH %% 7.0
# First, split the data randomly in half, into training and test sets. Use only the training set to fit
# your models, and use only the test set to evaluate how well they fit.
sample <- sample.int(n = nrow(data), size = floor(.5*nrow(data)), replace = F)
train <- data[sample, ]
test  <- data[-sample, ]

# NOTE call with a formula and use newdata to get predictions at the newdata
# http://www2.stat.duke.edu/~cr173/Sta444_Sp17/slides/Lec3.pdf

```

## a)

**Suppose we use an additive model where each additive function is a univariate kernel
smoother.Do you expect this model would have more or less bias and variance than the full kernel
smoother? Be specific about both the bias and variance.**

For efficicency I used only 1000 rows of the data. Coefficients are given in Table 1. We see that the mean squared error is 2503.734.

```{r, warning=FALSE, results='asis'}
set.seed(1234)
rows=sample(nrow(train), 1000)

```

## b) 

**Split the data into training and test sets, as you did in Homework 6. You should not need
to subsample the data, as additive models are quite fast.
Run the same baseline linear regression you used in Homework 6, so we have a basis to
compare against.
Using the gam function in the mgcv package, fit an additive model to the training data using
the default smoother for each term. (This means using a formula like y ~ s(x1) + s(x2) + ....
The s() model term smooths the variable using thin plate regression splines by default.
Note that it only makes sense to smooth over continuous variables; you can use ordinary
terms for categorical variables.)
(You should smooth over day of week and day of month. But note that you may need to
set the k argument to s() for day of week; by default, mgcv tries to use a spline basis with
more basis functions than there are unique days, which doesn’t work.)
Use predict and compare the squared-error loss of the additive model to that of the linear
model. Does the additive model do dramatically better?**

Coefficients are given in Table 2. We see that the mean squared error is 3309.925. This method does not seem to do better based on mse.

```{r, warning=FALSE, results='asis'}
bw =npregbw(ARR_DELAY~ DEP_TIME +as.factor(ORIGIN) +
              DAY_OF_MONTH +as.factor(CARRIER) + 
              as.factor(DAY_OF_WEEK),
            data=train[rows,])

loclinfit <- npreg(bw)
summary(loclinfit)

# mean squared-error loss
mean((test$ARR_DELAY - predict(loclinfit, newdata = test))^2, 
     na.rm = TRUE)
```

## c) 

**The plot method for GAM fits from mgcv plots the smoothed features automatically, in-
cluding standard errors. Make the plots and interpret the results. What do the plots sug-
gest about the appropriateness of linear regression? Do the plots explain the difference in performance between lienar regression and the additive model? Be sure to interpret
the standard error ranges and say what their widths imply.**

The plots below suggest that:

1. delay length (ARR_DELAY) is correlated with the variable ORIGIN. Seems like delay is longer from ORD.
2. delay length seems to decrease non linearly with DEP_TIME.

However the nonlinearities are slight and apparently don't seem to impair the performance of the ordinary linear regression for the subset of data we are using.

```{r, warning=FALSE, fig.height=8}
par(mfrow=c(3,2))
npplot(bw, 
       data =data, plot.errors.method= "bootstrap")
```

## d) 

**Repeat this analysis, but use a locally linear kernel regression (with regtype = "ll" provided to npregbw). Compare the test error from this model to that for the previous one. Discuss possible reasons for any difference you see.**

Here it seems that the mean squared error is 3749.95 which is larger than the previous model. This suggests that the locally linear version doesn't perform as well as the local-constant kernel, maybe due to overfitting to the training data. We have also seen the linear model does not seem to perform worse using the training data that we do have.

```{r, warning=FALSE, results='asis', fig.height=8}
bw2=npregbw(ARR_DELAY~DEP_TIME +as.factor(ORIGIN) +
              DAY_OF_MONTH +as.factor(CARRIER) +
              as.factor(DAY_OF_WEEK), 
            data=train[rows,],
            regtype = "ll")

loclinfit <- npreg(bw2)
summary(loclinfit)

# squared-error loss
mean((test$ARR_DELAY - predict(loclinfit, newdata = test))^2, na.rm = TRUE)

dev.new(width=5, height=8, unit = "in")
par(mfrow=c(5,1))
npplot(bw2,data = data,
       plot.errors.method= "bootstrap")
```