---
title: "HW7 q3"
date: "10/7/2019"
output: pdf_document
---


```{r setup, include=FALSE}
# libraries
library(tidyverse)
library(lattice)
library(dagitty)
library(reshape2)
library(grid)
library(gridExtra)
library(np)
library(stargazer)
library(knitr)
library(mgcv)
suppressWarnings(suppressMessages(library(stargazer))) 
knitr::opts_chunk$set(warning=FALSE,width.cutoff=60, echo = TRUE)
opts_knit$set(eval.after = 'fig.cap')

#   This question uses the following dataset on airline flight delays: http://rosmarus.refsmmat.com/datasets/datasets/flight-delays/
#   Download DFW_ORD_2016_12.csv, the subset of the data that only contains Dallas/Fort Worth
# and Chicago O’Hare. Also, install and load the np package for nonparametric r=egression;
# the example file I used in class, kernel-smoothing-density.Rmd, is posted on Canvas under
# Files→Activities.

# data
data <- read.csv("../data/DFW_ORD_2016_12.csv",
                     sep = ",", 
                     header = T)

#  Also, make a “day of week” variable by taking the remainder of dividing “day of month” by 7
data$DAY_OF_WEEK <- data$DAY_OF_MONTH %% 7.0
# First, split the data randomly in half, into training and test sets. Use only the training set to fit your models, and use only the test set to evaluate how well they fit.
sample <- sample.int(n = nrow(data), size = floor(.5*nrow(data)), replace = F)
train <- data[sample, ]
test  <- data[-sample, ]

# NOTE call with a formula and use newdata to get predictions at the newdata
# http://www2.stat.duke.edu/~cr173/Sta444_Sp17/slides/Lec3.pdf

```

## a)

**Suppose we use an additive model where each additive function is a univariate kernel smoother. Do you expect this model would have more or less bias and variance than the full kernel smoother? Be specific about both the bias and variance.**

The additive model should have higher bias than the full kernel smoother since each of the functions is a kernel with its own parameters. However there will be lower variance since the additive model is less flexible than the full kernel smoother.

## b) 
**Split the data into training and test sets, as you did in Homework 6. You should not need to subsample the data, as additive models are quite fast. Use predict and compare the squared-error loss of the additive model to that of the linear model. Does the additive model do dramatically better?**

The mse of the additive model is 2255.892 which is better than 2435.305 for the linear model. However this is not a dramatic difference.

```{r, warning=FALSE, results='asis'}
# Run the same baseline linear regression you used in Homework 6, so we have a basis to
# compare against. 
set.seed(1234)
base=lm(ARR_DELAY~ DEP_TIME +as.factor(ORIGIN) +
              DAY_OF_MONTH +as.factor(CARRIER) + 
              DAY_OF_WEEK, data=train) #[rows,]
summary(base)

#Using the gam function in the mgcv package, fit an additive model to the training data using the default smoother for each term. (This means using a formula like y ~ s(x1) + s(x2) + .... The s() model term smooths the variable using thin plate regression splines by default. Note that it only makes sense to smooth over continuous variables; you can use ordinary terms for categorical variables.)
additive <- mgcv::gam(ARR_DELAY~s(DEP_TIME) +as.factor(ORIGIN) +
              s(DAY_OF_MONTH) +as.factor(CARRIER) +
              s(DAY_OF_WEEK, k=7),
                      #family=Gamma(link=), 
                      data =train)

# (You should smooth over day of week and day of month. But note that you may need to
# set the k argument to s() for day of week; by default, mgcv tries to use a spline basis with more basis functions than there are unique days, which doesn’t work.)

# Use predict and compare the squared-error loss of the additive model to that of the linear model
mean((test$ARR_DELAY - predict.lm(base, newdata = test))^2, na.rm = TRUE) #2435.305

mean((test$ARR_DELAY - predict.gam(additive, newdata = test))^2, na.rm = TRUE) # 2133.988
```

## c) 

**The plot method for GAM fits from mgcv plots the smoothed features automatically, in-
cluding standard errors. Make the plots and interpret the results. What do the plots sug-
gest about the appropriateness of linear regression? Do the plots explain the difference in performance between lienar regression and the additive model? Be sure to interpret
the standard error ranges and say what their widths imply.**

If linear regression is appropriate, there should be some linearity in the response variable. Also the additive model should have higher variance and lower bias than the base linear model.

The plots of the smoothed features show that 

```{r, warning=FALSE, results='asis', fig.height=8}
# base plot
# par(mfrow=c(3,2))
# plot(summary(base), data =data)

# additive plot
par(mfrow=c(2,2))
plot.gam(additive)
```