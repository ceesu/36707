\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={HW9},
            pdfauthor={Cathy Su},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{HW9}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Cathy Su}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{8/11/2019}


\begin{document}
\maketitle

\subsection{Q2}\label{q2}

\subsubsection{(a)}\label{a}

\textbf{Use a support vector machine to classify emails as spam or ham.
The e1071 package on CRAN provides a svm function that will be useful.
Start by using the default parameters of the svm function and report
your test-set accuracy.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# data}
\NormalTok{spam <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"../data/gam-spam.csv"}\NormalTok{, }\DataTypeTok{header=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{spam}\OperatorTok{$}\NormalTok{V58 <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(spam}\OperatorTok{$}\NormalTok{V58)}
\NormalTok{training_rows <-}\StringTok{ }\KeywordTok{sample.int}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(spam), }\KeywordTok{round}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(spam) }\OperatorTok{/}\StringTok{ }\DecValTok{3}\NormalTok{))}

\NormalTok{spam.train <-}\StringTok{ }\NormalTok{spam[training_rows, ]}
\NormalTok{spam.test <-}\StringTok{ }\NormalTok{spam[}\OperatorTok{-}\NormalTok{training_rows, ]}

\CommentTok{# apply log transform}
\NormalTok{cols <-}\StringTok{ }\KeywordTok{names}\NormalTok{(spam.train)[}\DecValTok{1}\OperatorTok{:}\NormalTok{(}\KeywordTok{length}\NormalTok{(spam.train) }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{)]}
\ControlFlowTok{for}\NormalTok{ (col }\ControlFlowTok{in}\NormalTok{ cols) \{}
\NormalTok{  spam.train[col] <-}\StringTok{ }\KeywordTok{log}\NormalTok{(}\FloatTok{0.1} \OperatorTok{+}\StringTok{ }\NormalTok{spam.train[[col]])}
\NormalTok{  spam.test[col] <-}\StringTok{ }\KeywordTok{log}\NormalTok{(}\FloatTok{0.1} \OperatorTok{+}\StringTok{ }\NormalTok{spam.test[[col]])}
\NormalTok{\}}
\CommentTok{# }
\NormalTok{fit <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(V58 }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ spam.train)}
\CommentTok{#summary(fit)}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit, }\DataTypeTok{newdata =}\NormalTok{ spam.test, }\DataTypeTok{probability =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{table}\NormalTok{(spam.test}\OperatorTok{$}\NormalTok{V58, pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    pred
##        0    1
##   0    0 1844
##   1    0 1223
\end{verbatim}

\paragraph{(b)}\label{b}

\textbf{By default, svm uses a radial kernel, but it also supports
linear, polynomial, and sigmoid kernels. Each has different tuning
parameters, such as the degree of the polynomial kernel or gamma.
perform cross-validation to select the best tuning parameters. Build the
best classifier for each kernel, tuning over reasonable ranges of the
tuning pa- rameters, and report your results. Which kernel performs best
on this data? Which kernel performs worst, and why might it be the
worst?}

These are the tuning parameters for each kernel:

\begin{itemize}
\tightlist
\item
  radial: cost, gamma
\item
  linear: cost
\item
  polynomial: cost, coef0, degree, gamma
\item
  sigmoid: cost, coef0, gamma
\end{itemize}

I found that tuning the parameters in different function calls vs
together seemed to give the same results. Therefore I tuned once, and
then printed the outputs as below. The best performing kernel seems to
be the

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{type <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"linear"}\NormalTok{, }\StringTok{"polynomial"}\NormalTok{,}\StringTok{"sigmoid"}\NormalTok{, }\StringTok{"radial"}\NormalTok{)}

\NormalTok{params <-}\StringTok{ }\KeywordTok{tune.svm}\NormalTok{(V58 }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{spam.train, }
                   \DataTypeTok{cost=}\DecValTok{10}\OperatorTok{^}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{4}\NormalTok{), }
                   \DataTypeTok{gamma=}\DecValTok{10}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\OperatorTok{:-}\DecValTok{1}\NormalTok{), }
                   \DataTypeTok{coef0=}\DecValTok{10}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{),}
                   \DataTypeTok{degree=}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{)}

\NormalTok{####### polynomial}
\ControlFlowTok{for}\NormalTok{(k }\ControlFlowTok{in}\NormalTok{ type)\{}
  \KeywordTok{print}\NormalTok{(k)}
\NormalTok{  fit <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(V58 }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ spam.train,}
             \DataTypeTok{kernel =}\NormalTok{ k,}
             \DataTypeTok{degree=}\NormalTok{params}\OperatorTok{$}\NormalTok{best.parameter[[}\DecValTok{1}\NormalTok{]],}
             \DataTypeTok{cost=}\NormalTok{params}\OperatorTok{$}\NormalTok{best.parameter[[}\DecValTok{4}\NormalTok{]],}
             \DataTypeTok{coef0=}\NormalTok{params}\OperatorTok{$}\NormalTok{best.parameter[[}\DecValTok{3}\NormalTok{]], }
             \DataTypeTok{gamma=}\NormalTok{params}\OperatorTok{$}\NormalTok{best.parameter[[}\DecValTok{2}\NormalTok{]],}
             \DataTypeTok{probability =} \OtherTok{TRUE}\NormalTok{)}
  \CommentTok{#summary(fit)}
\NormalTok{  pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit, }\DataTypeTok{newdata =}\NormalTok{ spam.test)}
\NormalTok{  t <-}\StringTok{ }\KeywordTok{table}\NormalTok{(spam.test}\OperatorTok{$}\NormalTok{V58, pred)}\OperatorTok{/}\KeywordTok{length}\NormalTok{(pred)}
  \KeywordTok{print}\NormalTok{(}\KeywordTok{kable}\NormalTok{(t))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "linear"
## 
## 
##               0           1
## ---  ----------  ----------
## 0     0.5679817   0.0332573
## 1     0.0322791   0.3664819
## [1] "polynomial"
## 
## 
##               0           1
## ---  ----------  ----------
## 0     0.5758070   0.0254320
## 1     0.0384741   0.3602869
## [1] "sigmoid"
## 
## 
##               0           1
## ---  ----------  ----------
## 0     0.5735246   0.0277144
## 1     0.0388001   0.3599609
## [1] "radial"
## 
## 
##               0           1
## ---  ----------  ----------
## 0     0.5787414   0.0224976
## 1     0.0348875   0.3638735
\end{verbatim}

\subsubsection{(c)}\label{c}

\textbf{Compare the
accuracyyouhaveobtainedheretotheaccuracyyouobtainedonHomework 8 while
using random forests}

We see that based on the confusion matrix of each type, the

\subsection{Q3. Degrees of freedom of a
tree.}\label{q3.-degrees-of-freedom-of-a-tree.}

\textbf{An Introduction to Statistical Learning, chapter 9, exercise 5
(pp.~369--370). For part (i), when commenting on your results, answer
these questions: * In what space is the decision boundary linear? * If
we can make logistic regression produce nonlinear decision boundaries by
adding ap- propriate covariates, why is the kernel trick in SVMs
useful?}

\textbf{We have seen that we can fit an SVM with a non-linear kernel in
order to perform classification using a non-linear decision boundary. We
will now see that we can also obtain a non-linear decision boundary by
performing logistic regression using non-linear transformations of the
features.}

\subparagraph{a) Generate a data set with n = 500 and p = 2, such that
the obser- vations belong to two classes with a quadratic decision
boundary between them. For instance, you can do this as
follows:}\label{a-generate-a-data-set-with-n-500-and-p-2-such-that-the-obser--vations-belong-to-two-classes-with-a-quadratic-decision-boundary-between-them.-for-instance-you-can-do-this-as-follows}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1=}\KeywordTok{runif}\NormalTok{(}\DecValTok{500}\NormalTok{)}\OperatorTok{-}\FloatTok{0.5}
\NormalTok{x2=}\KeywordTok{runif}\NormalTok{(}\DecValTok{500}\NormalTok{)}\OperatorTok{-}\FloatTok{0.5}
\NormalTok{y=}\DecValTok{1}\OperatorTok{*}\NormalTok{(x1}\OperatorTok{^}\DecValTok{2}\OperatorTok{-}\NormalTok{x2}\OperatorTok{^}\DecValTok{2} \OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subparagraph{b) Generate 100 observations with predictors X1, X2, . . .
, X10 as inde- pendent standard Gaussian variates and fix these
values.}\label{b-generate-100-observations-with-predictors-x1-x2-.-.-.-x10-as-inde--pendent-standard-gaussian-variates-and-fix-these-values.}

\subparagraph{\texorpdfstring{c) Generate response values also as
standard Gaussian (\(\sigma^2\) = 1), indepen- dent of the predictors.
Fit regression trees to the data of fixed size 1,5 and 10 terminal nodes
and hence estimate the degrees of freedom of each fit. {[}Do ten
simulations of the response and average the results, to get a good
estimate of degrees of
freedom.{]}}{c) Generate response values also as standard Gaussian (\textbackslash{}sigma\^{}2 = 1), indepen- dent of the predictors. Fit regression trees to the data of fixed size 1,5 and 10 terminal nodes and hence estimate the degrees of freedom of each fit. {[}Do ten simulations of the response and average the results, to get a good estimate of degrees of freedom.{]}}}\label{c-generate-response-values-also-as-standard-gaussian-sigma2-1-indepen--dent-of-the-predictors.-fit-regression-trees-to-the-data-of-fixed-size-15-and-10-terminal-nodes-and-hence-estimate-the-degrees-of-freedom-of-each-fit.-do-ten-simulations-of-the-response-and-average-the-results-to-get-a-good-estimate-of-degrees-of-freedom.}

\subparagraph{d) Compare your estimates of degrees of freedom in (a) and
(c) and
discuss.}\label{d-compare-your-estimates-of-degrees-of-freedom-in-a-and-c-and-discuss.}

\subparagraph{e) Suggest a way to compute an approximate S matrix for a
regression tree, compute it and compare the resulting degrees of freedom
to those in (a) and
(c).}\label{e-suggest-a-way-to-compute-an-approximate-s-matrix-for-a-regression-tree-compute-it-and-compare-the-resulting-degrees-of-freedom-to-those-in-a-and-c.}

\subsection{Q4}\label{q4}

\textbf{Should we set C large or small if we want our estimated boundary
to have the minimum possible variance?}

If the parameter C is large, then any wrong classification is penalized
a lot. This would lead to higher variance since the classification
boundary tries to eliminate every single outlier. Thus to have smaller
variance we should set C to be small.

\subsection{Q5}\label{q5}

\textbf{Unbalanced classes. Simulate a dataset with two variables, X1
and X2, and an outcome variable Y. Make the dataset have a linear
decision boundary between Y = 1 and Y = 0. But put the code that
simulates data into a function, and make one of the parameters of that
function be the fraction of cases that should have Y = 1. This way, we
can simulate what happens when the classes are imbalanced.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulate <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{fraction=}\FloatTok{0.5}\NormalTok{) \{}
\NormalTok{  X <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\DecValTok{2} \OperatorTok{*}\StringTok{ }\NormalTok{N, }\DataTypeTok{min=}\DecValTok{0}\NormalTok{, }\DataTypeTok{max=}\DecValTok{10}\NormalTok{), }\DataTypeTok{nrow=}\NormalTok{N, }\DataTypeTok{ncol=}\DecValTok{2}\NormalTok{)}
\NormalTok{  Y <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{((X[, }\DecValTok{1}\NormalTok{] }\OperatorTok{<}\StringTok{ }\DecValTok{5} \OperatorTok{&}\StringTok{ }\NormalTok{X[, }\DecValTok{2}\NormalTok{] }\OperatorTok{<}\StringTok{ }\DecValTok{5}\NormalTok{) }\OperatorTok{|}\StringTok{ }\NormalTok{(X[, }\DecValTok{1}\NormalTok{] }\OperatorTok{>}\StringTok{ }\DecValTok{5} \OperatorTok{&}\StringTok{ }\NormalTok{X[, }\DecValTok{2}\NormalTok{] }\OperatorTok{>}\StringTok{ }\DecValTok{5}\NormalTok{), }\KeywordTok{rbinom}\NormalTok{(N, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }\KeywordTok{rbinom}\NormalTok{(N, }\DecValTok{1}\NormalTok{, }\FloatTok{0.9}\NormalTok{))}
  
  \KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(X) }\OperatorTok{==}\StringTok{ }\KeywordTok{length}\NormalTok{(Y)) }\CommentTok{# sanity check!}
\NormalTok{  new <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{as.data.frame}\NormalTok{(X), }\KeywordTok{as.dsata.frame}\NormalTok{(Y))}
\NormalTok{  new}\OperatorTok{$}\NormalTok{Y <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(new}\OperatorTok{$}\NormalTok{Y)}
  \CommentTok{#colnames(new)[-1] <- "Y"}
  \KeywordTok{return}\NormalTok{(new)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\subsubsection{a)}\label{a-1}

\textbf{Set the fraction to 0.5: perfect balance. Run a logistic
regression and an SVM on the data to predict Y. Compare the confusion
matrices on a test set (just run your function again to get a test
set!). How do the error rates compare, and do the error rates differ for
points with Y = 0 vs.~those with Y = 1?}

\subsubsection{b)}\label{b-1}

\textbf{Now repeat this analysis with fractions varying from 0.6 to
0.95. Look at the confusion matrices as the classes become less
balanced.What happens to the errors for logistic regression? What kinds
of errors become more common?}

\subsubsection{c)}\label{c-1}

\textbf{What happens to the errors for SVMs in the same case? Show a
graph or table summarizing the results and comparing SVMs to logistic
regression.}

\subsubsection{d)}\label{d}

\textbf{Describe, in words, why the difference you observed should
exist. Give your explanation in terms of how SVMs and logistic
regression find the decision boundary, and what loss functions they
use.}


\end{document}
