---
title: "HW6 q2"
date: "10/7/2019"
output: pdf_document
---


```{r setup, include=FALSE}
# libraries
library(tidyverse)
library(lattice)
library(dagitty)
library(reshape2)
library(grid)
library(gridExtra)
library(np)
library(stargazer)
library(knitr)
suppressWarnings(suppressMessages(library(stargazer))) 
knitr::opts_chunk$set(warning=FALSE,width.cutoff=60, echo = TRUE)
opts_knit$set(eval.after = 'fig.cap')

#   This question uses the following dataset on airline flight delays: http://rosmarus.refsmmat.com/datasets/datasets/flight-delays/
#   Download DFW_ORD_2016_12.csv, the subset of the data that only contains Dallas/Fort Worth
# and Chicago O’Hare. Also, install and load the np package for nonparametric r=egression;
# the example file I used in class, kernel-smoothing-density.Rmd, is posted on Canvas under
# Files→Activities.

# data
data <- read.csv("../data/DFW_ORD_2016_12.csv",
                     sep = ",", 
                     header = T)

#  Also, make a “day of week” variable by taking the remainder of dividing “day of month” by 7
data$DAY_OF_WEEK <- data$DAY_OF_MONTH %% 7.0
# First, split the data randomly in half, into training and test sets. Use only the training set to fit
# your models, and use only the test set to evaluate how well they fit.
sample <- sample.int(n = nrow(data), size = floor(.5*nrow(data)), replace = F)
train <- data[sample, ]
test  <- data[-sample, ]

# NOTE call with a formula and use newdata to get predictions at the newdata
# http://www2.stat.duke.edu/~cr173/Sta444_Sp17/slides/Lec3.pdf

```

## a)

**We are interested in predicting arrival delay (ARR_DELAY) using the variables that would
be available before the flight takes off: departure time, departure airport (ORD or DFW),
day of the month, and air carrier (airline). Also, make a “day of week” variable.
First, for a baseline comparison, fit a linear model to the data using these variables. (Be
naive: assume the effects of time of day or day of week are linear.) Show a table of the
coefficients you find. Evaluate the model on the test set (using predict) and report its
squared-error loss.**

For efficicency I used only 1000 rows of the data. Coefficients are given in Table 1. We see that the mean squared error is 2503.734.

```{r, warning=FALSE, results='asis'}
set.seed(1234)
rows=sample(nrow(train), 1000)
# fit on the training data
best.fit=lm(ARR_DELAY~ DEP_TIME +as.factor(ORIGIN) +
              DAY_OF_MONTH +as.factor(CARRIER) + 
              as.factor(DAY_OF_WEEK), data=train[rows,])
#coef(best.fit, 8)
stargazer(best.fit, 
          title = "Q2a",
          header=FALSE, type = "latex", font.size="small",
          column.sep.width = "1pt",
          single.row = TRUE)

# squared-error loss
mean((test$ARR_DELAY - predict.lm(best.fit, newdata = test))^2, na.rm = TRUE)
```

## b) 

**The relationships may be complicated and a linear model may not be appropriate, so
use npreg to fit a Nadaraya–Watson kernel regression model; allow npregbw to select all
bandwidths with cross-validation.
Report the kernel regression’s performance on the test set (again using predict and squared-error loss) and compare to the linear model. Does this method seem to do dra-
matically better?**

Coefficients are given in Table 2. We see that the mean squared error is 3309.925. This method does not seem to do better based on mse.

```{r, warning=FALSE, results='asis'}
bw =npregbw(ARR_DELAY~ DEP_TIME +as.factor(ORIGIN) +
              DAY_OF_MONTH +as.factor(CARRIER) + 
              as.factor(DAY_OF_WEEK),
            data=train[rows,])

loclinfit <- npreg(bw)
summary(loclinfit)

# mean squared-error loss
mean((test$ARR_DELAY - predict(loclinfit, newdata = test))^2, 
     na.rm = TRUE)
```

## c) 

**The plot function for npregression objects (such as the fit returned by npreg) can plot
the marginal association of each variable with the response. If you set the plot.errors.method = "bootstrap" option, it will also plot bootstrap-based standard errors for these. Make the plots with standard errors and interpret the results. Which variables seem strongly related with delay length? What do the plots suggest about the appropriate-ness of linear regression? If you saw major non-linearities in any variable, do these non-
linearities appear to harm the predictions enough to make linear regression perform dra-
matically worse than kernel regression?**

The plots below suggest that:

1. delay length (ARR_DELAY) is correlated with the variable ORIGIN. Seems like delay is longer from ORD.
2. delay length seems to decrease non linearly with DEP_TIME.

However the nonlinearities are slight and apparently don't seem to impair the performance of the ordinary linear regression for the subset of data we are using.

```{r, warning=FALSE, fig.height=8}
par(mfrow=c(3,2))
npplot(bw, 
       data =data, plot.errors.method= "bootstrap")
```

## d) 

**Repeat this analysis, but use a locally linear kernel regression (with regtype = "ll" provided to npregbw). Compare the test error from this model to that for the previous one. Discuss possible reasons for any difference you see.**

Here it seems that the mean squared error is 3749.95 which is larger than the previous model. This suggests that the locally linear version doesn't perform as well as the local-constant kernel, maybe due to overfitting to the training data. We have also seen the linear model does not seem to perform worse using the training data that we do have.

```{r, warning=FALSE, results='asis', fig.height=8}
bw2=npregbw(ARR_DELAY~DEP_TIME +as.factor(ORIGIN) +
              DAY_OF_MONTH +as.factor(CARRIER) +
              as.factor(DAY_OF_WEEK), 
            data=train[rows,],
            regtype = "ll")

loclinfit <- npreg(bw2)
summary(loclinfit)

# squared-error loss
mean((test$ARR_DELAY - predict(loclinfit, newdata = test))^2, na.rm = TRUE)

dev.new(width=5, height=8, unit = "in")
par(mfrow=c(5,1))
npplot(bw2,data = data,
       plot.errors.method= "bootstrap")
```