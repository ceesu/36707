---
title: "HW9"
author: "Cathy Su"
date: "8/11/2019"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
---
```{r global_options, include=FALSE}
# knitr::knit_hooks$set(plot = function(x, options)  {
#   hook_plot_tex(x, options)
# })
library(knitr)
opts_knit$set(eval.after = 'fig.cap')
```

```{r setup, include=FALSE}
#http://rosmarus.refsmmat.com/datasets/datasets/hormone-diversity/ 
  
knitr::opts_chunk$set( warning=FALSE,width.cutoff=60)
set.seed(1234)

# libraries
library(stargazer)
library(tidyverse)
library(lattice)
library(dagitty)
library(reshape2)
library("ggpubr")
library(knitr)
library(ranger)
library("e1071")
```

## Q2 

### (a) 

**Use a support vector machine to classify emails as spam or ham. The e1071 package on CRAN provides a svm function that will be useful. Start by using the default parameters of the svm function and report your test-set accuracy.**

```{r}
# data
spam <- read.csv("../data/gam-spam.csv", header=FALSE)
spam$V58 <- as.factor(spam$V58)
training_rows <- sample.int(nrow(spam), round(nrow(spam) / 3))

spam.train <- spam[training_rows, ]
spam.test <- spam[-training_rows, ]

# apply log transform
cols <- names(spam.train)[1:(length(spam.train) - 1)]
for (col in cols) {
  spam.train[col] <- log(0.1 + spam.train[[col]])
  spam.test[col] <- log(0.1 + spam.test[[col]])
}
# 
fit <- svm(V58 ~ ., data = spam.train)
#summary(fit)
pred <- predict(fit, newdata = spam.test, probability = TRUE)
table(spam.test$V58, pred)
```

#### (b) 

**By default, svm uses a radial kernel, but it also supports linear, polynomial, and sigmoid kernels. Each has different tuning parameters, such as the degree of the polynomial kernel or ð›¾ in the radial basis. Fortunately, e1071 provides the tune.svm function to automa cally perform cross-validation to select the best tuning parameters. Build the best classifier for each kernel, tuning over reasonable ranges of the tuning pa- rameters, and report your results. Which kernel performs best on this data? Which kernel performs worst, and why might it be the worst?**

SThe best performing kernel is
```{r}
type <- c("linear", "polynomial","sigmoid", "radial")

f# tune 
params <- tune.svm(label~., data=data,
                                    gamma=10^(-6:-2), cost=10^(1:2))

or(k in type){
  print(k)
  ftune.svm(spam.train, spam.train, kernel = k)
  it <- svm(V58 ~ ., data = spam.train,
             kernel = k)
  #summary(fit)
  pred <- predict(fit, newdata = spam.test)
  table(spam.test$V58, pred)
}

```
### (c) 
**Compare the accuracyyouhaveobtainedheretotheaccuracyyouobtainedonHomework 8 while using random forests**

Here we used the impurity measure to assess importance, which calculates importance "as the sum over the number of splits (accross all tress

## Q5

Unbalanced classes. Simulate a dataset with two variables, ð‘‹1 and ð‘‹2, and an outcome vari- able ð‘Œ âˆˆ {0, 1}. Make the dataset have a linear decision boundary between ð‘Œ = 1 and ð‘Œ = 0. But put the code that simulates data into a function, and make one of the parameters of that function be the fraction of cases that should have ð‘Œ = 1. This way, we can simulate what happens when the classes are imbalanced.

### (a) 
**Set the fraction to 0.5: perfect balance. Run a logistic regression and an SVM on the data to predict ð‘Œ. Compare the confusion matrices on a test set (just run your function again to get a test set!). How do the error rates compare, and do the error rates differ for points with ð‘Œ = 0 vs. those with ð‘Œ = 1?**

We see from the normalized confusion matrix that the error rate is high, at about 50%.

```{r}

make_data <- function(N=150) {
  X <- matrix(runif(2 * N, min=0, max=10), nrow=N, ncol=2)
  Y <- ifelse((X[, 1] < 5 & X[, 2] < 5) | (X[, 1] > 5 & X[, 2] > 5), rbinom(N, 1, 0.1), rbinom(N, 1, 0.9))
  
  stopifnot(nrow(X) == length(Y)) # sanity check!
  new <- cbind(as.data.frame(X), as.data.frame(Y))
  new$Y <- as.factor(new$Y)
  #colnames(new)[-1] <- "Y"
  return(new)
}

spam <- make_data(N=800)
training_rows <- sample.int(nrow(spam), round(nrow(spam) / 3))

train <- spam[training_rows, ]
test <- spam[-training_rows, ]

fit <- ranger(Y ~ ., data = train, max.depth = 1)
pred <- predict(fit, data = test)

table(test$Y, pred$predictions)/length(pred$predictions)
```

### (b) 

**Now repeat this analysis with fractions varying from 0.6 to 0.95. Look at the confusion matrices as the classes become less balanced. What happens to the errors for logistic regression? What kinds of errors become more common? Be sure to distinguish between predicting ð‘ŒÌ‚ = 0 when ð‘Œ = 1 from predicting ð‘ŒÌ‚ = 1 when ð‘Œ = 0**

We see that the error decreases to around 10% when depth > 4. This depth is a bit larger than we might have expected based on the in class activity (2), but it could be because this allows splitting along each of the two main dimensions multiple times. 

Increased depth does not seem to lead to higher error rate in the test set. There might still be harm from allowing trees with such large depth that they could dramatically increase the runtime. 

```{r}
# Plot test error against depth
depths =c(1:50)

######## Compare the accuracy on the test set for all the combinations.
err <-numeric(length(depths))
for (m in 1:length(depths)){
  fit <- ranger(Y ~ .,
                max.depth=depths[m],
                data = train)
  
  # find the accuracy
  pred <- predict(fit, data = test, se.fit=TRUE)
  err[m] <- 1-length(test$Y[test$Y==pred$predictions])/length(test$Y)
}

#par(mfrow=c(2,2))
plot(depths, err, main = "Percent error")
```


### (c) 

**What happens to the errors for SVMs in the same case? Show a graph or table summariz- ing the results and comparing SVMs to logistic regression.**

In our dataset we have 800 samples, with a 75:25 train:test split and we found that:


### (d) 

**Describe, in words, why the difference you observed should exist. Give your explanation in terms of how SVMs and logistic regression find the decision boundary, and what loss functions they use.**

In our dataset we have 800 samples, with a 75:25 train:test split and we found that:



## Q3. 

**An Introduction to Statistical Learning, chapter 9, exercise 5 (pp. 369â€“370). For part (i), when commenting on your results, answer these questions:
* In what space is the decision boundary linear?
* If we can make logistic regression produce nonlinear decision boundaries by adding ap-
propriate covariates, why is the kernel trick in SVMs useful?**

## Q4

**Should we set C large or small if we want our estimated boundary to have the minimum possible variance?**

The parameter C

## Q5

**Unbalanced classes. Simulate a dataset with two variables, X1 and X2, and an outcome variable Y. Make the dataset have a linear decision boundary between Y = 1 and Y = 0. But put the code that simulates data into a function, and make one of the parameters of that function be the fraction of cases that should have Y = 1. This way, we can simulate what happens when the classes are imbalanced.**

```{r}
simulate <- function(fraction=0.5) {
  X <- matrix(runif(2 * N, min=0, max=10), nrow=N, ncol=2)
  Y <- ifelse((X[, 1] < 5 & X[, 2] < 5) | (X[, 1] > 5 & X[, 2] > 5), rbinom(N, 1, 0.1), rbinom(N, 1, 0.9))
  
  stopifnot(nrow(X) == length(Y)) # sanity check!
  new <- cbind(as.data.frame(X), as.dsata.frame(Y))
  new$Y <- as.factor(new$Y)
  #colnames(new)[-1] <- "Y"
  return(new)
}
```


### a)

**Set the fraction to 0.5: perfect balance. Run a logistic regression and an SVM on the data to predict Y. Compare the confusion matrices on a test set (just run your function again to get a test set!). How do the error rates compare, and do the error rates differ for points with Y = 0 vs. those with Y = 1?**


### b)

**Now repeat this analysis with fractions varying from 0.6 to 0.95. Look at the confusion matrices as the classes become less balanced.What happens to the errors for logistic regression? What kinds of errors become more common?**

### c)

**What happens to the errors for SVMs in the same case? Show a graph or table summarizing the results and comparing SVMs to logistic regression.**

### d)

**Describe, in words, why the difference you observed should exist. Give your explanation in terms of how SVMs and logistic regression find the decision boundary, and what loss functions they use.**