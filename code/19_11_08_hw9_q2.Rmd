---
title: "HW9"
author: "Cathy Su"
date: "8/11/2019"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
---
```{r global_options, include=FALSE}
# knitr::knit_hooks$set(plot = function(x, options)  {
#   hook_plot_tex(x, options)
# })
library(knitr)
opts_knit$set(eval.after = 'fig.cap')
```

```{r setup, include=FALSE}
#http://rosmarus.refsmmat.com/datasets/datasets/hormone-diversity/ 
  
knitr::opts_chunk$set( warning=FALSE,width.cutoff=60)
set.seed(1234)

# libraries
library(stargazer)
library(tidyverse)
library(lattice)
library(dagitty)
library(reshape2)
library("ggpubr")
library(knitr)
library(ranger)
library("e1071")
```

## Q2 

### (a) 

**Use a support vector machine to classify emails as spam or ham. The e1071 package on CRAN provides a svm function that will be useful. Start by using the default parameters of the svm function and report your test-set accuracy.**

```{r}
# data
spam <- read.csv("../data/gam-spam.csv", header=FALSE)
spam$V58 <- as.factor(spam$V58)
training_rows <- sample.int(nrow(spam), round(nrow(spam) / 3))

spam.train <- spam[training_rows, ]
spam.test <- spam[-training_rows, ]

# apply log transform
cols <- names(spam.train)[1:(length(spam.train) - 1)]
for (col in cols) {
  spam.train[col] <- log(0.1 + spam.train[[col]])
  spam.test[col] <- log(0.1 + spam.test[[col]])
}
# 
fit <- svm(V58 ~ ., data = spam.train)
#summary(fit)
pred <- predict(fit, newdata = spam.test, probability = TRUE)
table(spam.test$V58, pred)
```

#### (b) 

**By default, svm uses a radial kernel, but it also supports linear, polynomial, and sigmoid kernels. Each has different tuning parameters, such as the degree of the polynomial kernel or gamma. perform cross-validation to select the best tuning parameters. Build the best classifier for each kernel, tuning over reasonable ranges of the tuning pa- rameters, and report your results. Which kernel performs best on this data? Which kernel performs worst, and why might it be the worst?**

These are the tuning parameters for each kernel:

* radial: cost, gamma
* linear: cost
* polynomial: cost, coef0, degree,  gamma
* sigmoid: cost, coef0,  gamma

I found that tuning the parameters in different function calls vs together seemed to give the same results. Therefore I tuned and then printed the outputs as below. The best performing kernel is

```{r}
type <- c("linear", "polynomial","sigmoid", "radial")

####### LINEAR
print(type[1])
# tune 
params <- tune.svm(V58 ~ ., data=spam.train, 
                   cost=10^(0:4)) 
                   #gamma=0.5, 
                   #coef0=0.0,
                   #degree=2)
fit <- svm(V58 ~ ., data = spam.train,
           kernel = type[1],
           #degree=params$best.parameter[[1]],
           cost=params$best.parameter[[1]]
           #coef0=params$best.parameter[[3]], 
           #gamma=params$best.parameter[[2]])
           )
#summary(fit)
pred <- predict(fit, newdata = spam.test)
table(spam.test$V58, pred)/length(pred)

####### polynomial
print(type[2])
# tune 
params <- tune.svm(V58 ~ ., data=spam.train, 
                   cost=10^(0:4), 
                   gamma=10^(-4:-1), 
                   coef0=0.0,
                   degree=1:4)
fit <- svm(V58 ~ ., data = spam.train,
           kernel = type[2],
           degree=params$best.parameter[[1]],
           cost=params$best.parameter[[4]],
           coef0=params$best.parameter[[3]], 
           gamma=params$best.parameter[[2]])
#summary(fit)
pred <- predict(fit, newdata = spam.test)
table(spam.test$V58, pred)/length(pred)

####### polynomial
print(type[3])
# tune 
params <- tune.svm(V58 ~ ., data=spam.train, 
                   cost=10, 
                   gamma=0.5, 
                   coef0=0.0,
                   degree=2)
fit <- svm(V58 ~ ., data = spam.train,
           kernel = k,
           degree=params$best.parameter[[1]],
           cost=params$best.parameter[[4]],
           coef0=params$best.parameter[[3]], 
           gamma=params$best.parameter[[2]])
#summary(fit)
pred <- predict(fit, newdata = spam.test)
table(spam.test$V58, pred)/length(pred)


####### polynomial
print(type[4])
# tune 
params <- tune.svm(V58 ~ ., data=spam.train, 
                   cost=10, 
                   gamma=0.5, 
                   coef0=0.0,
                   degree=2)
fit <- svm(V58 ~ ., data = spam.train,
           kernel = k,
           degree=params$best.parameter[[1]],
           cost=params$best.parameter[[4]],
           coef0=params$best.parameter[[3]], 
           gamma=params$best.parameter[[2]])
#summary(fit)
pred <- predict(fit, newdata = spam.test)
table(spam.test$V58, pred)/length(pred)

```
### (c) 
**Compare the accuracyyouhaveobtainedheretotheaccuracyyouobtainedonHomework 8 while using random forests**

We see that based on the confusion matrix, 


## Q3. 

**An Introduction to Statistical Learning, chapter 9, exercise 5 (pp. 369â€“370). For part (i), when commenting on your results, answer these questions:
* In what space is the decision boundary linear?
* If we can make logistic regression produce nonlinear decision boundaries by adding ap-
propriate covariates, why is the kernel trick in SVMs useful?**

## Q4

**Should we set C large or small if we want our estimated boundary to have the minimum possible variance?**

The parameter C

## Q5

**Unbalanced classes. Simulate a dataset with two variables, X1 and X2, and an outcome variable Y. Make the dataset have a linear decision boundary between Y = 1 and Y = 0. But put the code that simulates data into a function, and make one of the parameters of that function be the fraction of cases that should have Y = 1. This way, we can simulate what happens when the classes are imbalanced.**

```{r}
simulate <- function(fraction=0.5) {
  X <- matrix(runif(2 * N, min=0, max=10), nrow=N, ncol=2)
  Y <- ifelse((X[, 1] < 5 & X[, 2] < 5) | (X[, 1] > 5 & X[, 2] > 5), rbinom(N, 1, 0.1), rbinom(N, 1, 0.9))
  
  stopifnot(nrow(X) == length(Y)) # sanity check!
  new <- cbind(as.data.frame(X), as.dsata.frame(Y))
  new$Y <- as.factor(new$Y)
  #colnames(new)[-1] <- "Y"
  return(new)
}
```


### a)

**Set the fraction to 0.5: perfect balance. Run a logistic regression and an SVM on the data to predict Y. Compare the confusion matrices on a test set (just run your function again to get a test set!). How do the error rates compare, and do the error rates differ for points with Y = 0 vs. those with Y = 1?**


### b)

**Now repeat this analysis with fractions varying from 0.6 to 0.95. Look at the confusion matrices as the classes become less balanced.What happens to the errors for logistic regression? What kinds of errors become more common?**

### c)

**What happens to the errors for SVMs in the same case? Show a graph or table summarizing the results and comparing SVMs to logistic regression.**

### d)

**Describe, in words, why the difference you observed should exist. Give your explanation in terms of how SVMs and logistic regression find the decision boundary, and what loss functions they use.**