---
title: "hw5_q2"
date: "10/02/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, width.cutoff=60, fig.pos = 'H')
set.seed(1234)

# libraries
library(tidyverse)
library(lattice)
library(dagitty)
library(reshape2)
library(grid)
library(gridExtra)
library("ggpubr")

# http://rosmarus.refsmmat.com/datasets/datasets/college-scorecard/
data <- read.csv("~/36707/data/Most-Recent-Cohorts-Scorecard-Elements.csv",
                 na.strings = c("PrivacySuppressed", "NULL"), sep = ",")

# Note that missing data is marked with the string PrivacySuppressed.
# You should filter out colleges with missing data in the columns we will use for this problem.

```

## a) median SAT scores versus the median earnings after 10 years

Here we scatterplot the approximately linear relationship between these variables. First we removed the rows with missing values. Based on the line of best fit, there appear to be a number of high earnings outliers (red points). It seems the outliers belong to graduates of medical schools or prestigious schools who can earn more than average (printed below), suggesting the presence of other covariates responsible for the earnings seen here. To try to avoid these effects we removed outliers with earnings greater than 75000.

```{r outlier, fig.width=6, fig.height=4} 
# filter out missing data
filt <- data %>% 
  select(SATMTMID, MD_EARN_WNE_P10, INSTNM) %>%
  na.omit
ggscatter(filt, x = "SATMTMID", y = "MD_EARN_WNE_P10", #xlab = FALSE, ylab = FALSE,
               cor.coef = TRUE,  add = "reg.line", cor.method = "pearson")+
  geom_point(size=2,color = ifelse(filt$MD_EARN_WNE_P10 > 75000, "red", "blue"))
filt[filt$MD_EARN_WNE_P10 > 75000,"INSTNM"]
```

## b) median earnings and SAT math scores

Here we fit an ordinary linear model of median earnings vs median SAT math scores. The diagnostic plots suggest the fit is not very linear:

* The residuals vs fitted plot shows residuals have a downward trend rather than being equally spread relative to the fitted values.
* the QQ plot has many points which are not well aligned on the y=x line indicating some residuals are not normally distributed.
* the plot of residuals vs. leverage shows a couple of potentially problematic outliers with high residuals and/or leverage, lying close to the Cook's distance curves.

Overall the model relationship is not very linear (adjusted R-squared less than 0.2) and the diagnostic plots show some problems with the fit.
```{r pressure, echo=FALSE}
# follow https://data.library.virginia.edu/diagnostic-plots/
filt_data <- data[data$MD_EARN_WNE_P10 >75000,] %>% 
  select(SATMTMID, MD_EARN_WNE_P10, INSTNM)  %>% na.omit()
   
mod <- lm(MD_EARN_WNE_P10 ~ SATMTMID, data = filt_data)
summary(mod)

# plot residuals and diagnostics
par(mfrow = c(2, 2)) 
plot(mod)
```

## c) Nonlinear fits

We fit a second-order and third-order polynomial here and find that neither of these provide a statistically significant improvement over the original first order linear model by partial F test (no significant terms at p < 0.05). The diagnostic plots also do not show improvements from previous ones. For example the third order polynomial diagnostic plot of residuals versus leverage (shown below) even see the appearance of an outlier that has moved further outside of the Cook's distance lines (labelled by 1550). This suggests the nonlinear fits are comparable to the linear fit.

```{r nonlinear}
filt_data <- data[data$MD_EARN_WNE_P10 >75000,] %>% 
  select(SATMTMID, MD_EARN_WNE_P10, INSTNM) %>% na.omit()
   
mod2 <- lm(MD_EARN_WNE_P10 ~ poly(SATMTMID, 2), data = filt_data)
summary(mod2)

mod3 <- lm(MD_EARN_WNE_P10 ~ poly(SATMTMID, 3), data = filt_data)
summary(mod3)
# plot residuals and diagnostics
par(mfrow = c(2, 2)) 
plot(mod3)

# Use partial F tests to compare the models
anova(mod, mod2)
anova(mod, mod3)
```

## d) Cross validation 

```{r cv}
library(modelr)
cv <- crossv_kfold(filt_data)

model1 <- map(cv$train, ~lm(MD_EARN_WNE_P10 ~ SATMTMID, data =.))
model2 <- map(cv$train, ~lm(MD_EARN_WNE_P10 ~ poly(SATMTMID, 2), data =.))
model3 <- map(cv$train, ~lm(MD_EARN_WNE_P10 ~ poly(SATMTMID, 3), data =.))

# Use cross-validation to estimate the squared-error loss of each of your models.
errs1 <- map2_dbl(model1, cv$test, mse)
errs2 <- map2_dbl(model2, cv$test, mse)
errs3 <- map2_dbl(model3, cv$test, mse)

# print mean mse
mean(as.numeric(errs1))
mean(as.numeric(errs2))
mean(as.numeric(errs3))
```
We checked the cross validation MSE and found that the predictive performance of the original linear model is better than the performance of the order 2 polynomial, which is in turn better than performance of the order 3 polynomial. This is different from our conclusion from the F tests since it suggests the simpler models outperform teh more complex ones.

The conclusion can be different because the F test tells us that the hypothesis that the more complex model performs better cannot be rejected; whereas cross validation mean squared error characterizes the predictive performance of the model dependent on the training and test data we have.

## e) Fit a smoothing spline 

For the same model as above we compared the following three spline models:

* cv.fit: spar picked by automatic cross-validation
* half_cv.fit: spar set to be half as big as R picked
* half.fit: spar set to be halfway between cv.fit and 1

```{r splines}
library(splines)
# fit the splines
cv.fit <- smooth.spline(filt_data$SATMTMID,
                        filt_data$MD_EARN_WNE_P10)
spar_fit <- cv.fit$spar #spar= 1.499929
spar_fit 
half_cv.fit <- smooth.spline(filt_data$SATMTMID,
                             filt_data$MD_EARN_WNE_P10,
                             spar = spar_fit/2.0)
half.fit <- smooth.spline(filt_data$SATMTMID,
                          filt_data$MD_EARN_WNE_P10,
                          spar = (spar_fit-1.0)/2.0 + 1.0)

# Plot predictions from the three models on one scatterplot and compare them visually.
# sat math scores can range from 200 to 800
satmids <- seq(200, 800, length.out=100)

plot(filt_data[,1:2], xlab="Midpoint of SAT math scores", 
     ylab="Median earnings (dollars) 10 years after entry")
lines(satmids, predict(cv.fit, satmids)$y, col="red")
lines(satmids, predict(half_cv.fit, satmids)$y, col="blue")
lines(satmids, predict(half.fit, satmids)$y, col="brown")
legend(x= "topright", legend=c("cv.fit", "half_cv.fit", "half.fit"),
       col=c("red", "blue", "brown"), lty=1:2, cex=0.8)
```

From a visual comparison it's clear that whereas half.fit and cv.fit are very similar straight line models, half_cv.fit is a more fitted model with higher variance.

## (f) Use cross-validation to estimate the error of the three splines. 

It seems that based upon the MSE values seen from the three modes, high bias wins over high variance in cross validation performance. This is seen because the average mse of the half_cv spline (which has the smallest spar and higher variance than the other two models) is the largest.

```{r splines_cv}
library(modelr)
# make splits
k=5
folds <- cut(seq(1,nrow(filt_data)),breaks=k,labels=FALSE)
mse1 <- vector(mode = "numeric", length = k)
mse2 <- vector(mode = "numeric", length = k)
mse3 <- vector(mode = "numeric", length = k)
for(i in 1:k){
  testIndexes <- which(folds==i,arr.ind=TRUE)
  testData <- filt_data[testIndexes, 1:2]
  trainData <- filt_data[-testIndexes, 1:2]
  # get 3 model on specific train fold
  cv.fit <- smooth.spline(trainData$SATMTMID,
                        trainData$MD_EARN_WNE_P10,
                        spar = spar_fit)
  half_cv.fit <- smooth.spline(trainData$SATMTMID,
                             trainData$MD_EARN_WNE_P10,
                             spar = spar_fit/2.0)
  half.fit <- smooth.spline(trainData$SATMTMID,
                          trainData$MD_EARN_WNE_P10,
                          spar = (spar_fit-1.0)/2.0 + 1.0)
  # extract model performance on test fold
  y <- testData$MD_EARN_WNE_P10
  yhat1 <- predict(cv.fit, testData$SATMTMID)$y
  yhat2 <- predict(half_cv.fit,testData$SATMTMID)$y
  yhat3 <- predict(half.fit, testData$SATMTMID)$y
  
  # get the mse 
  mse1[i] <- mean((y-yhat1)^2)
  mse2[i] <- mean((y-yhat2)^2)
  mse3[i] <- mean((y-yhat3)^2)
  }
# Compare the average errors of the three.

mean(as.numeric(mse1))
mean(as.numeric(mse2))
mean(as.numeric(mse3))
```