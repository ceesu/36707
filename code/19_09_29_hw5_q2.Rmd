---
title: "hw5_q2"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, width.cutoff=60, fig.pos = 'H')
set.seed(1234)

# libraries
library(tidyverse)
library(lattice)
library(dagitty)
library(reshape2)
library(grid)
library(gridExtra)
library("ggpubr")

# http://rosmarus.refsmmat.com/datasets/datasets/college-scorecard/
data <- read.csv("~/36707/data/Most-Recent-Cohorts-Scorecard-Elements.csv",
                 na.strings = c("PrivacySuppressed", "NULL"), sep = ",")

# Note that missing data is marked with the string PrivacySuppressed.
# You should filter out colleges with missing data in the columns we will use for this problem.

```

## a) SATMTMID vs MD_EARN_WNE_P10
Here we plot median SAT scores versus the median earnings after 10 years which has an approximately linear relationship (r^2 > 0.6). First we removed the rows with missing values. Based on the line of best fit, outliers appeared to have earnings greater than 75000 (red points). It seems the outliers belong to graduates of medical schools or prestigious schools who can earn more than average (printed below). I removed these outliers.

```{r outlier} 
# filter out missing data
filt <- data %>% 
  select(SATMTMID, MD_EARN_WNE_P10, INSTNM) %>%
  na.omit
ggscatter(filt, x = "SATMTMID", y = "MD_EARN_WNE_P10", #xlab = FALSE, ylab = FALSE,
               cor.coef = TRUE,  add = "reg.line", cor.method = "pearson")+
  geom_point(size=2,color = ifelse(filt$MD_EARN_WNE_P10 > 75000, "red", "blue"))
filt[filt$MD_EARN_WNE_P10 > 75000,"INSTNM"]
```

## b) median earnings and SAT math scores

Here we fit an ordinary linear model of median earnings vs median SAT math scores. The R squared values and diagnostic plots suggest the fit is not very linear.

* The residuals vs fitted plot shows residuals have a downward trend rather than being equally spread relative to the fitted values.
* the QQ plot has many points which are not well aligned on the y=x line indicating some residuals are not normally distributed.
* the plot of residuals vs. leverage shows a couple of potentially problematic outliers with high residuals and/or leverage, lying close to the Cook's distance curves.

Overall the model relationship is not very linear (Adjusted R-squared of 0.1097) and the diagnostic plots show some problems with the fit.
```{r pressure, echo=FALSE}
# follow https://data.library.virginia.edu/diagnostic-plots/
filt_data <- data[data$MD_EARN_WNE_P10 >75000,] %>% 
  select(SATMTMID, MD_EARN_WNE_P10, INSTNM)  %>% na.omit()
   
mod <- lm(MD_EARN_WNE_P10 ~ SATMTMID, data = filt_data)
summary(mod)

# plot residuals and diagnostics
par(mfrow = c(2, 2)) 
plot(mod)
```

## c) Nonlinear fits

We fit a second-order and third-order polynomial instead and find that neither of these provide a statistically significant improvement over the original first order linear model by partial F test (no significant terms at p < 0.05). The diagnostic plots also do not show improvements from previous ones. For example the third order polynomial diagnostic plots shown below even see the appearance of an outlier that has moved further outside of the Cook's distance lines. This suggests the nonlinear fits are comparable to the linear fit.

```{r nonlinear}
filt_data <- data[data$MD_EARN_WNE_P10 >75000,] %>% 
  select(SATMTMID, MD_EARN_WNE_P10, INSTNM) %>% na.omit()
   
mod2 <- lm(MD_EARN_WNE_P10 ~ poly(SATMTMID, 2), data = filt_data)
summary(mod2)

mod3 <- lm(MD_EARN_WNE_P10 ~ poly(SATMTMID, 3), data = filt_data)
summary(mod3)
# plot residuals and diagnostics
par(mfrow = c(2, 2)) 
plot(mod3)

# Use partial F tests to compare the models
anova(mod, mod2)
anova(mod, mod3)
```

## d) Cross validation 

```{r cv}
library(modelr)

folds <- cut(seq(1,nrow(filt_data)),breaks=5,labels=FALSE)

testIndexes <- which(folds==i,arr.ind=TRUE)
testData <- filt_data[testIndexes, 1:2]
trainData <- filt_data[-testIndexes, 1:2]
model1 <- map(cv$train, ~lm(MD_EARN_WNE_P10 ~ SATMTMID, data =.))
model2 <- map(cv$train, ~lm(MD_EARN_WNE_P10 ~ poly(SATMTMID, 2), data =.))
model3 <- map(cv$train, ~lm(MD_EARN_WNE_P10 ~ poly(SATMTMID, 3), data =.))

# Use cross-validation to estimate the squared-error loss of each of your models.
errs1 <- map2_dbl(model1, testData, mse)
errs2 <- map2_dbl(model2, testData, mse)
errs3 <- map2_dbl(model3, testData, mse)

# print mean mse
mean(as.numeric(errs1))
mean(as.numeric(errs2))
mean(as.numeric(errs3))
```

Whether or not the F tests suggested the polynomials help, you can also compare the pre-
dictive performance of each model. Sometimes variables that are not statistically signifi-
cant can improve predictive performance. Use cross-validation to estimate the squared-
error loss of each of your models. (Fit a new model to each training set.) Compare the
results to what you got using F tests. Why could the results differ?

(For K-fold cross-validation, the modelr package has a crossv_kfold function that auto-
matically divides your data up into folds, and gives you lists of the training and test sets.

We checked the cross validation RSS and found that the predictive performance of the original linear model is better than the performance of the order 2 polynomial, which is in turn better than performance of the order 3 polynomial. This is different from our conclusion from the F tests.

## e) Fit a smoothing spline 

For the same model as above we compared the following three spline models:

* cv.fit: spar picked by automatic cross-validation
* half_cv.fit: spar set to be half as big as R picked
* half.fit: spar set to be halfway between cv.fit and 1

Whereas half.fit and cv.fit are very similar, half_cv.fit is a less linear model.

```{r splines}
library(splines)
# fit the splines
cv.fit <- smooth.spline(filt_data$SATMTMID,
                        filt_data$MD_EARN_WNE_P10)
spar_fit <- cv.fit$spar #spar= 1.499929
spar_fit 
half_cv.fit <- smooth.spline(filt_data$SATMTMID,
                             filt_data$MD_EARN_WNE_P10,
                             spar = spar_fit/2.0)
half.fit <- smooth.spline(filt_data$SATMTMID,
                          filt_data$MD_EARN_WNE_P10,
                          spar = (spar_fit-1.0)/2.0 + 1.0)

# Plot predictions from the three models on one scatterplot and compare them visually.
# sat math scores can range from 200 to 800
satmids <- seq(200, 800, length.out=100)

plot(filt_data[,1:2], xlab="Midpoint of SAT math scores", 
     ylab="Median earnings (dollars) 10 years after entry")
lines(satmids, predict(cv.fit, satmids)$y, col="red")
lines(satmids, predict(half_cv.fit, satmids)$y, col="blue")
lines(satmids, predict(half.fit, satmids)$y, col="brown")
legend(x= "topright", legend=c("cv.fit", "half_cv.fit", "half.fit"),
       col=c("red", "blue", "brown"), lty=1:2, cex=0.8)
```

## (f) Use cross-validation to estimate the error of the three splines. 

 Which is worse, too much bias or too much
variance? (Which fit corresponds to high bias, and which corresponds to high variance?)


```{r splines_cv}
library(modelr)
# make splits
k=5
folds <- cut(seq(1,nrow(filt_data)),breaks=k,labels=FALSE)
mse1 <- vector(mode = "numeric", length = k)
mse2 <- vector(mode = "numeric", length = k)
mse3 <- vector(mode = "numeric", length = k)
for(i in 1:k){
  testIndexes <- which(folds==i,arr.ind=TRUE)
  testData <- filt_data[testIndexes, 1:2]
  trainData <- filt_data[-testIndexes, 1:2]
  # get 3 model on specific train fold
  cv.fit <- smooth.spline(trainData$SATMTMID,
                        trainData$MD_EARN_WNE_P10,
                        spar = spar_fit)
  half_cv.fit <- smooth.spline(trainData$SATMTMID,
                             trainData$MD_EARN_WNE_P10,
                             spar = spar_fit/2.0)
  half.fit <- smooth.spline(trainData$SATMTMID,
                          trainData$MD_EARN_WNE_P10,
                          spar = (spar_fit-1.0)/2.0 + 1.0)
  # extract model performance on test fold
  y <- testData$MD_EARN_WNE_P10
  yhat1 <- predict(cv.fit, testData$SATMTMID)$y
  yhat2 <- predict(half_cv.fit,testData$SATMTMID)$y
  yhat3 <- predict(half.fit, testData$SATMTMID)$y
  
  # get the mse 
  mse1[i] <- mean((y-yhat1)^2)
  mse2[i] <- mean((y-yhat2)^2)
  mse3[i] <- mean((y-yhat3)^2)
  }
# Compare the average errors of the three.

mean(as.numeric(mse1))
mean(as.numeric(mse2))
mean(as.numeric(mse3))
```