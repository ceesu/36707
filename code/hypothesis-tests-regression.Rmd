---
title: "Hypothesis tests in regression"
author: "Qiao Su"
date: "10/21/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is a fill-in-the-blank activity. Read the text, run the code, and follow the instructions, writing your answers where blanks are provided. You'll then turn this in as a short "homework" for credit.

We've seen a lot about hypothesis tests in regression: *t* tests, *F* tests, and other tests. What exactly do they mean, and what do they measure?

We'll explore this using some simulations. In each simulation, we'll make *X* using random uniform variables that are not correlated with each other?

```{r}
# Generate random X with N rows and p columns.
generate_X <- function(N, p) {
  X <- matrix(runif(N * p), nrow=N)

  return(as.data.frame(X))
}
```

Here are functions to generate Y from X using beta:

```{r}
c
generate_error <- function(X, sigma) {
  return(sigma * rnorm(nrow(X)))
}

# Generate Y using X, beta, and the error.
generate_Y <- function(X, beta, sigma) {
  return(as.matrix(X) %*% beta + generate_error(X, sigma))
}
```

These functions will make it easy for us to extract the F statistic and $R^2$ for a model:

```{r}
# Return the p value for the F test of a linear model.
f_p_value <- function(fit) {
  fstat <- summary(fit)$fstatistic
  
  pf(fstat["value"], fstat["numdf"], fstat["dendf"], lower.tail=FALSE)
}

# Return the R^2 for a linear model fit.
r_squared <- function(fit) {
  summary(fit)$r.squared
}
```


## Power of tests

Let's imagine running an ordinary linear model with real effects. What sample size do we need to detect those effects?

The probability of detecting a certain-size effect with a certain sample size is called the *power*. If the power is low, then our failure to detect a statistically significant difference doesn't mean much -- if a true effect were there, we quite likely wouldn't have found it.

Try running the simulation below. Examine the plot produced and make sure you understand what the code is doing.

```{r}
beta <- c(-0.2, 0, -0.8)
sigma <- 0.5 # scale teh normal distribution

# What sample sizes should we try?
#sample_sizes <- 10:100
beta_scale <- seq(0.4, 2, by=0.2)
sigma_scale <- seq(0.4, 2, by=0.2)

# For each sample size, what fraction of the tests were significant?
num_significant <- numeric(length(beta_scale))

# How many times should we try at each sample size?
num_trials <- 100
N <- 50

for (jj in seq_along(sigma_scale)) {
 # beta <- beta*beta_scale[jj]/1.0
  sigma <- sigma*sigma_scale[jj]
  
  for (ii in 1:num_trials) {
    X <- generate_X(N, length(beta))
    Y <- generate_Y(X, beta, sigma)
  
    fit <- lm(Y ~ ., data=X) # . means "all columns"
    
    if (f_p_value(fit) <= 0.05) {
      num_significant[jj] <- num_significant[jj] + 1
    }
  }
}

plot(sigma_scale, num_significant / num_trials,
     xlab="sigma_scale", ylab="Power to reject F test null")
```

### Questions for you

Now, answer these questions *before* trying out more simulations. Write your answers below.

1. If the coefficients in beta are larger (in absolute value), but we keep N the same, should the power increase or decrease? Why does this make sense?
2. If sigma is larger, keeping N the same, should the power increase or decrease?
3. If we fail to reject the F test, what is a reasonable conclusion to draw? Write out a sentence of the form "Since p > 0.05, we... and conclude that..."

Answers:

1. The power should increase because we would need to show that the effect is still distinguishable from zero.
2. The power decreases since the signal is noisier. 
3. Since p > 0.05, we cannot reject the null hypothesis and conclude that the coefficient beta is not significantly different from zero with a probability of 95%. We cannot distinguish whether the coefficients have an effect.
However we can say that the effect size is not easy to detect.

Now try running a few more simulations (just tweak the code above) to answer questions 1 and 2. Summarize your results:

1. The power increases with increasing beta in a nonlinear way.
2. The power decreases with increasing sigma in a nonlinear way. It drops off sharply after sigma scaling factor increases past about 1.5 fold.

## The effect of doing model selection

Let's consider doing a similar simulation. But this time, beta will be a zero vector: there is **no** effect. We run stepwise variable selection and then consider the *F* test of the selected model.

Crucially, we'll add more variables -- variables that still have no true effect.

**Before** you run the simulation below:

Question: If we add more variables, meaning more columns in *X*, but none of them are correlated with *Y*, what will happen to the *F* test after stepwise selection? Will it conclude that there are significant effects?

Answer: It will conclude that there are significant effects, because we are able to pick variables which have small p-values. As the number of variables increases the number of significant effects will increase linearly.

```{r}
sigma <- 0.5

# Sample size to try -- needs to be bigger than the number of variables
N <- 100

# How many variables to use in our model
num_variables <- seq(1, 40, by=2)

# For each number of variables, what fraction of the tests were significant?
num_significant <- numeric(length(num_variables))

# How many times should we try?
num_trials <- 50

# what is r squared?
rsq <- numeric(length(num_variables))

for (jj in seq_along(num_variables)) {
  p <- num_variables[jj]

  for (ii in 1:num_trials) {
    X <- generate_X(N, p)
    
    # Don't bother using generate_Y, because beta is 0. It would just do a matrix
    # multiplication with 0 for no purpose.
    Y <- generate_error(X, sigma)
    
    fit <- lm(Y ~ ., data=X)
    # stepwise regression 
    #step_fit <- step(fit, trace=0) # do stepwise regression
    
    # calculate r squared
    rsq[jj] <- rsq[jj]+ summary(fit)$r.squared
    
    #if (length(coef(step_fit)) > 1 && f_p_value(step_fit) <= 0.05) {
    # if (length(coef(fit)) > 1 && f_p_value(fit) <= 0.05) {
    #   num_significant[jj] <- num_significant[jj] + 1
    # }
  }
}

rsq <- rsq/num_trials # take the average of all the r squareds
plot(num_variables,rsq,  #num_significant / num_trials,
     xlab="Number of variables", ylab="Fraction of nulls falsely rejected")
```

It seems like from the plot that the fraction of nulls falsely rejected increases almost linearly with the number of variables.

### Questions for you

Now, some questions. Answer these **before** running any more simulations:

1. What will happen if we *don't* do stepwise selection, and just fit the model to all the variables? Will we see the same relationship in the graph?
2. The $R^2$ statistic summarizes the amount of variance in *Y* that is explained by the columns in *X*. In this simulation, none of the variation in *Y* is truly related to *X*, but some variation will be related by chance. How will $R^2$ vary with the number of variables in the model?

Answers:
1. We will not, because where stepwise selection would pick out the best variables for every number of variables, in this case there's a random chance that any of the variables at each step would be found as a significant explanatory variable.
2. It does seem to increase because more variables are able to explain more of the varation just by chance. 

Now adjust the simulations above and write what you find:

1. It seems like the fraction of falsely rejected nulls does not increase with hte number of variables included, and the relationship is noisier rather than linear.
2. It seems like it does increase linearly and consistently with number of variables.

## Non-normal errors

Next question: What is the effect of non-Normal residuals on regression, and specifically on the hypothesis tests we do? We know the F test assumes normality of residuals, but how much does that matter?

We can make our errors be heavy-tailed by using a *t* distribution:

```{r}
# Generate errors for all X. The errors are t-distributed with 2 degrees of freedom.
generate_error_t <- function(X, sigma) {
  return(sigma * rt(nrow(X), df=2)) # rt generates random deviates.
}
```

What does this look like in a QQ plot? Run this a few times to see some examples.
 Ans: there are outlier residuals at the two ends of the qq plot. 

```{r}
data <- rt(100, df=2)
qqnorm(data)
qqline(data)
```

Let's fit models at different sample sizes to find distribution of pvals:

```{r}
# https://stackoverflow.com/questions/5587676/pull-out-p-values-and-r-squared-from-a-linear-regression
lmp <- function (modelobject) {
    if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
    f <- summary(modelobject)$fstatistic
    p <- pf(f[1],f[2],f[3],lower.tail=F)
    attributes(p) <- NULL
    return(p)
}

sigma <- 0.5

# How many variables to use in our model
p <- 3
N <- c(5, 300) # try a large N and small N

# How many times should we try?
num_trials <- 500

for (jj in N) {
  X <- generate_X(jj, p)
  pvals_tdist <- numeric(length(num_trials))
  pvals_ndist <- numeric(length(num_trials))
  for (ii in 1:num_trials) {
    Y_tdist <- generate_error_t(X, sigma)/sqrt(jj)
    #Y_tdist <- Y_tdist/max(Y_tdist) #scale t distribution so it has the same standard deviation
    Y_ndist <- generate_error(X, sigma)
    fit_t <- lm(Y_tdist ~ ., data=X)
    fit_n <- lm(Y_ndist ~ ., data=X)
    
    pvals_tdist[ii] <- f_p_value(fit_t)
    pvals_ndist[ii] <- f_p_value(fit_n)
  }
  # now make a plot of the pvals.
  hist(pvals_tdist)
  hist(pvals_ndist)
  
  plot(d, col = "red", main = paste0("pval distribution for N=", jj))
  lines(d2)
 legend = c("bottomleft", legend= c("tdist", "ndist"))
 # title(main = paste0("pval distribution for N=", jj))
}
```

Now plot the number of rejected nulls:

```{r}
sigma <- 0.5

# For each number of variables, what fraction of the tests were significant?
num_significant <- numeric(length(num_variables))
num_significant_t <- numeric(length(num_variables))
# How many variables to use in our model
p <- 3
N <- seq(p, 500, by=5) # Sample size
n_pvals_tdist <- numeric(length(N))
n_pvals_ndist <- numeric(length(N))

for (jj in length(N)) {
  n <- N[jj]
  for (ii in 1:num_trials) {
    X <- generate_X(n, p)
    
    Y_tdist <- generate_error_t(X, sigma)/sqrt(n)#scale t distribution so it has the same standard deviation 
    Y_ndist <- generate_error(X, sigma)
    
    fit_t <- lm(Y_tdist ~ ., data=X)
    fit_n <- lm(Y_ndist ~ ., data=X)

    if (length(coef(fit_t)) > 1 && f_p_value(fit_t) <= 0.05) {
      num_significant_t[jj] <- num_significan_tt[jj] + 1
    }
    if (length(coef(fit_n)) > 1 && f_p_value(fit_n) <= 0.05) {
      num_significant_t[jj] <- num_significant_t[jj] + 1
    }
  }
}

# Do you observe any differences in the behavior of the hypothesis test?
     #xlab="N", ylab="Density")

# What do the effects of non-normality appear to be on the F test?
plot( num_significant_t / num_trials,
     xlab="Number of variables", ylab="Fraction of nulls falsely rejected")
lines(num_significant / num_trials)
 legend = c("bottomleft", legend= c("tdist", "ndist"))

```

**Conduct simulations with p = 3 and N ranging from the smallest possible value to larger values, say N = 500. Look at the distribution of F test pvalues in each situation, both with t-distributed errors and with normally-distributed errors. (Ensure you scale the t distribution so it has the same standard deviation as the normal distribution!)**

1. Do you observe any differences in the behavior of the hypothesis tests? 

With p = 3 and N as 5 or 500, I plotted the distribution of F test pvalues for each fit (in the model with tdistributed errors, and in the model with normally distributed errors) on the same graph. The distributions are similar with some differences; they are more bit more different at N=5 than at N=500.

2. What do the effects of non-normality appear to be on the F test?

Looking at the same plot, it appears the pvalues from fitting the t-distributed errors are very similar, and then  
